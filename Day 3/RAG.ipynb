{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdCkoqbCWZNs",
        "outputId": "a4cc7b70-0ae3-4fd5-9ddd-ac3faed11a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch faiss-cpu pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQFEKZm1bjFe",
        "outputId": "9e006714-8035-4b34-e7b2-d6bda582a4b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import fitz  # PyMuPDF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# === CONFIG ===\n",
        "DATA_FOLDER = \"/content/drive/MyDrive/RAG pdf/Financial Report - IBM.pdf\"\n",
        "CHUNK_SIZE = 400\n",
        "CHUNK_OVERLAP = 50\n",
        "TOP_K = 3\n",
        "\n",
        "# === TEXT CHUNKING ===\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# === LOAD FILES (.txt + .pdf) ===\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    return \" \".join([page.get_text() for page in doc])\n",
        "\n",
        "def load_documents(folder):\n",
        "    documents = []\n",
        "    if not os.path.exists(folder):\n",
        "        raise FileNotFoundError(f\"Folder '{folder}' not found. Please add files.\")\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        elif filename.endswith(\".pdf\"):\n",
        "            text = extract_text_from_pdf(filepath)\n",
        "        else:\n",
        "            continue  # skip unsupported\n",
        "\n",
        "        if text.strip():\n",
        "            chunks = chunk_text(text)\n",
        "            for chunk in chunks:\n",
        "                documents.append({\"text\": chunk, \"source\": filename})\n",
        "    return documents\n",
        "\n",
        "# === BUILD FAISS INDEX ===\n",
        "def build_faiss_index(documents, model):\n",
        "    texts = [doc[\"text\"] for doc in documents]\n",
        "    embeddings = model.encode(texts)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(embeddings).astype(\"float32\"))\n",
        "    return index, np.array(embeddings), documents\n",
        "\n",
        "# === RETRIEVE CHUNKS ===\n",
        "def retrieve_top_k(index, model, query, documents, top_k=TOP_K):\n",
        "    query_embedding = model.encode([query])\n",
        "    D, I = index.search(np.array(query_embedding).astype(\"float32\"), top_k)\n",
        "    return [documents[i] for i in I[0]]\n",
        "\n",
        "# === QA MODEL ===\n",
        "def generate_answer(chunks, query):\n",
        "    context = \" \".join([chunk[\"text\"] for chunk in chunks])\n",
        "    qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "    result = qa_model(question=query, context=context)\n",
        "    return result[\"answer\"], context\n",
        "\n",
        "# === SOURCE ATTRIBUTION ===\n",
        "def get_sources(chunks):\n",
        "    return list(set([chunk[\"source\"] for chunk in chunks]))\n",
        "\n",
        "# === MAIN RAG PIPELINE ===\n",
        "def rag_pipeline(query):\n",
        "    print(f\"\\nüîé Question: {query}\")\n",
        "    documents = load_documents(DATA_FOLDER)\n",
        "    if not documents:\n",
        "        raise ValueError(\"No valid documents found in the folder.\")\n",
        "\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    index, embeddings, docs = build_faiss_index(documents, embedding_model)\n",
        "    top_chunks = retrieve_top_k(index, embedding_model, query, docs)\n",
        "    answer, context = generate_answer(top_chunks, query)\n",
        "    sources = get_sources(top_chunks)\n",
        "\n",
        "    print(f\"\\n‚úÖ Answer: {answer}\")\n",
        "    print(\"üìö Sources:\")\n",
        "    for src in sources:\n",
        "        print(f\"- {src}\")\n",
        "\n",
        "# === TEST ===\n",
        "if __name__ == \"__main__\":\n",
        "    test_questions = [\n",
        "        \"What are the key components of a Transformer model?\",\n",
        "        \"How does positional encoding work in Transformers?\",\n",
        "        \"Explain few-shot learning with an example.\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        rag_pipeline(question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "8_qticnsc-k7",
        "outputId": "d7b5a102-4424-4ffc-81b0-6bd2a6029f22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Question: What are the key components of a Transformer model?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/drive/MyDrive/RAG pdf/Financial Report - IBM.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2358350483>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_questions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mrag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-2358350483>\u001b[0m in \u001b[0;36mrag_pipeline\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüîé Question: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid documents found in the folder.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2358350483>\u001b[0m in \u001b[0;36mload_documents\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Folder '{folder}' not found. Please add files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/RAG pdf/Financial Report - IBM.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (run once in terminal):\n",
        "# pip install sentence-transformers PyPDF2 scikit-learn transformers torch\n",
        "!pip install PyPDF2\n",
        "import os\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Document Preprocessing\n",
        "# ---------------------------\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def vectorize_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
        "    return embeddings, chunks\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Retriever\n",
        "# ---------------------------\n",
        "\n",
        "def retrieve_relevant_chunks(query, embeddings, chunks, model_name='all-MiniLM-L6-v2', top_k=3):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return [(chunks[i], similarities[i]) for i in top_indices]\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Answer Generation\n",
        "# ---------------------------\n",
        "\n",
        "def generate_answer(query, relevant_chunks, model_name=\"distilgpt2\"):\n",
        "    context = \"\\n\".join([chunk for chunk, _ in relevant_chunks])\n",
        "    generator = pipeline(\"text-generation\", model=model_name)\n",
        "    prompt = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
        "    result = generator(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']\n",
        "    return result.strip(), context\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Source Attribution\n",
        "# ---------------------------\n",
        "\n",
        "def source_attribution(relevant_chunks):\n",
        "    return [\n",
        "        f\"Source {i+1}: \\\"{chunk[:100]}...\\\" (Similarity: {score:.4f})\"\n",
        "        for i, (chunk, score) in enumerate(relevant_chunks)\n",
        "    ]\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Main Execution\n",
        "# ---------------------------\n",
        "\n",
        "def run_rag_system(query, pdf_paths):\n",
        "    all_chunks, all_embeddings = [], []\n",
        "\n",
        "    for path in pdf_paths:\n",
        "        print(f\"Processing: {path}\")\n",
        "        text = load_pdf(path)\n",
        "        chunks = chunk_text(text)\n",
        "        embeddings, _ = vectorize_chunks(chunks)\n",
        "        all_chunks.extend(chunks)\n",
        "        all_embeddings.append(embeddings)\n",
        "\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    relevant_chunks = retrieve_relevant_chunks(query, all_embeddings, all_chunks)\n",
        "    answer, context = generate_answer(query, relevant_chunks)\n",
        "    citations = source_attribution(relevant_chunks)\n",
        "\n",
        "    return answer, citations\n",
        "\n",
        "# ---------------------------\n",
        "# Example Usage\n",
        "# ---------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # üî∫ Make sure your PDF files are in the 'pdfs/' folder\n",
        "    pdf_folder = \"/content/drive/MyDrive/RAG Project Dataset pdf/RAG Project Dataset\"\n",
        "    pdf_paths = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
        "\n",
        "    # Sample question (you can modify)\n",
        "    query = \"What are the main components of a RAG model and how do they interact?\"\n",
        "\n",
        "    if not pdf_paths:\n",
        "        print(\"‚ùå No PDF files found in './pdfs'. Please add your research papers.\")\n",
        "    else:\n",
        "        answer, citations = run_rag_system(query, pdf_paths)\n",
        "        print(\"\\n‚úÖ Query:\", query)\n",
        "        print(\"\\nüìò Answer:\\n\", answer)\n",
        "        print(\"\\nüìå Citations:\")\n",
        "        for cite in citations:\n",
        "            print(cite)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNQJY9mRfqJf",
        "outputId": "9de919fc-ea76-4daf-c4b5-539c9b65c6cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Processing: /content/drive/MyDrive/RAG Project Dataset pdf/RAG Project Dataset/2005.14165v4.pdf\n",
            "Processing: /content/drive/MyDrive/RAG Project Dataset pdf/RAG Project Dataset/1706.03762v7.pdf\n",
            "Processing: /content/drive/MyDrive/RAG Project Dataset pdf/RAG Project Dataset/2005.11401v4.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Query: What are the main components of a RAG model and how do they interact?\n",
            "\n",
            "üìò Answer:\n",
            " Question: What are the main components of a RAG model and how do they interact?\n",
            "Context: retriever p\u0011(zjx)with parameters \u0011that returns (top-K truncated) distributions over text passages given a query xand (ii) a generator p\u0012(yijx;z;y 1:i\u00001)parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by\u0012that generates a current token based on a context of the previous i\u00001tokensy1:i\u00001, the original inputxand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the model uses the same document to predict each target token. The second approach, RAG-Token , can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\u0011andp\u0012components, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(yjx)via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence (yjx)\u0019X z2top-k(p(\u0001jx))p\u0011(zjx)p\u0012(yjx;z) =X z2top-k(p(\u0001jx))p\u0011(zjx)NY ip\u0012(yijx;z;y 1:i\u00001) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne: pRAG-Token (yjx)\u0019NY iX z2top-k(p(\u0001jx))p\u0011(zjx)p\u0012(yijx;z;y 1:i\u00001) Finally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component p\u0011(zjx)is based on DPR [26]. DPR follows a bi-encoder architecture: p\u0011(zjx)/exp\u0000 d(z)>q(x)\u0001 d(z) =BERTd(z);q(x) =BERTq(x) where d(z)is a dense representation of a document produced by a BERT BASE document encoder [8], andq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating top-k (p\u0011(\u0001jx)), the list ofkdocumentszwith highest prior probability p\u0011(zjx), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory . 2.3 Generator: BART The generator component p\u0012(yijx;z;y 1:i\u00001)could be modelled using any encoder-decoder. We use BART-large [ 32], a pre-trained seq2seq transformer [ 58]\n",
            "documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiÔ¨Åcant differences in performance between them. We have the Ô¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50 KR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok RAG-Seq 10 20 30 40 50 KR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok RAG-Seq Fixed DPR BM25 10 20 30 40 50 KR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L RAG-Tok B-1 RAG-Seq R-L RAG-Seq B-1 Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor- mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our work uniÔ¨Åes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8 General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiÔ¨Åcation tasks in the GLUE bench- marks [ 60,61] after Ô¨Åne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniÔ¨Åed architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signiÔ¨Åcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some work optimizes the retrieval module to aid in a speciÔ¨Åc, downstream task such as question answering, using search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be Ô¨Åne-tuned for strong\n",
            "any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54]. Advanced language models may also lead to the automation of various jobs in the coming decades [ 16]. In order to mitigate these risks, AI systems could be employed to Ô¨Åght against misleading content and automated spam/phishing. Acknowledgments The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program. References [1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http: //arxiv.org/abs/1611.09268 . arXiv: 1611.09268. [2]Petr Baudi≈° and Jan ≈†ediv `y. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages , pages 222‚Äì228. Springer, 2015. URL https://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20 . [3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1533‚Äì1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D13-1160 . [4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod- ing&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159 . [5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1870‚Äì1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171 . [6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 209‚Äì220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 . 10 [7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre- hension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 . arXiv: 1710.10723. [8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\n",
            "Answer: on a particular context, the generator generates a new value and produces the current value as the new value. The generated values are used to generate an algorithm based on a context of the previous i.e. the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the new value of the original value of the original value of the original value of the original value of the new value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value of the original value\n",
            "\n",
            "üìå Citations:\n",
            "Source 1: \"retriever p\u0011(zjx)with parameters \u0011that returns (top-K truncated) distributions over text passages gi...\" (Similarity: 0.3771)\n",
            "Source 2: \"documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe s...\" (Similarity: 0.3714)\n",
            "Source 3: \"any potential external knowledge source, will probably never be entirely factual and completely devo...\" (Similarity: 0.3616)\n"
          ]
        }
      ]
    }
  ]
}